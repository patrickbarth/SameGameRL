{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-Based SameGame RL Experiment\n",
    "\n",
    "This notebook implements a Deep Q-Network (DQN) agent using a Convolutional Neural Network to learn optimal SameGame strategies. The CNN architecture is designed to capture spatial patterns in the game board that are crucial for effective tile-clearing decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "Setting up the basic experiment parameters and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"CNN_simple_reward_base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "\n",
    "Loading all necessary modules for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'asyncpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DqnAgent\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_evals, plot_result\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Benchmark\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "File \u001b[0;32m/notebooks/samegamerl/evaluation/benchmark.py:25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgreedy_singles_bot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GreedySinglesBot\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     GameSnapshot,\n\u001b[1;32m     22\u001b[0m     BotPerformance,\n\u001b[1;32m     23\u001b[0m     BenchmarkData,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark_repository_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     BenchmarkRepositoryFactory,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark_execution_strategies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     ExecutionStrategyFactory,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn_agent_benchmark_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DqnAgentBenchmarkAdapter\n",
      "File \u001b[0;32m/notebooks/samegamerl/evaluation/benchmark_repository_factory.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark_repository\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PickleBenchmarkRepository\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase_benchmark_repository\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseBenchmarkRepository\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark_repository_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BenchmarkRepositoryInterface\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgame\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgame_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GameConfig\n",
      "File \u001b[0;32m/notebooks/samegamerl/evaluation/database_benchmark_repository.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Database-backed repository for benchmark data with memory efficiency.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseRepository, GamePool\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BenchmarkData, BotPerformance, GameSnapshot\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmark_repository_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BenchmarkRepositoryInterface\n",
      "File \u001b[0;32m/notebooks/samegamerl/database/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Database package for SameGameRL benchmarking system.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseManager, db_manager\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Base, Bot, GamePool, Game, GameConfig, GameResult\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msamegamerl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatabase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepository\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseRepository\n",
      "File \u001b[0;32m/notebooks/samegamerl/database/connection.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncpg\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'asyncpg'"
     ]
    }
   ],
   "source": [
    "from samegamerl.environments.samegame_env import SameGameEnv\n",
    "from samegamerl.agents.dqn_agent import DqnAgent\n",
    "from samegamerl.evaluation.plot_helper import plot_evals, plot_result\n",
    "from samegamerl.evaluation.benchmark import Benchmark\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from samegamerl.game.game_config import GameConfig, GameFactory\n",
    "from samegamerl.training.train import train\n",
    "from samegamerl.agents.replay_buffer import ReplayBuffer\n",
    "from samegamerl.evaluation.benchmark_scripts import _compute_stats, benchmark_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "Defining a CNN model that processes the game board as a multi-channel image. The architecture uses:\n",
    "- Convolutional layers to detect local tile patterns\n",
    "- Global average pooling to aggregate spatial information\n",
    "- Fully connected layers for action value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, config: GameConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(config.num_colors, config.num_rows*config.num_cols, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d((2,2), (2,2)),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d((2,2), (2,2)),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, config.action_space_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Configuration of training parameters, exploration strategy, and reporting intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training specific parameters\n",
    "batch_size = 128\n",
    "n_games = 1_000\n",
    "max_steps = 30  # Maximum steps per episode\n",
    "\n",
    "# Training intervals\n",
    "update_target_num = 100    # Target network update frequency\n",
    "report_num = 500             # Progress reporting interval\n",
    "visualize_num = 0            # Visualization frequency\n",
    "initial_update_done = n_games // 2\n",
    "\n",
    "# Agent hyperparameters\n",
    "learning_rate = 0.001\n",
    "start_epsilon = 1.0           # Initial exploration rate\n",
    "epsilon_decay = start_epsilon / n_games\n",
    "final_epsilon = 0.1           # Minimum exploration rate\n",
    "gamma = 0.95                   # Discount factor\n",
    "tau = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Agent Setup\n",
    "\n",
    "Creating the SameGame environment and DQN agent with the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Use medium game configuration (8x8 board with 3 colors)\n",
    "config = GameFactory.medium()\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = SameGameEnv(config, partial_completion_base=5)\n",
    "agent = DqnAgent(\n",
    "    model=NeuralNetwork(config),\n",
    "    config=config,\n",
    "    model_name=experiment_name,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    "    gamma=gamma,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "agent.replay_buffer = ReplayBuffer(capacity=50_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model (Optional)\n",
    "\n",
    "Loading a previously trained model to continue training from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from samegamerl/models/CNN_simple_reward_base.pth\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to load a pre-trained model\n",
    "agent.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Execute the main training process using the configured parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(\n",
    "    agent,\n",
    "    env,\n",
    "    epochs=n_games,\n",
    "    max_steps=max_steps,\n",
    "    report_num=report_num,\n",
    "    visualize_num=visualize_num,\n",
    "    update_target_num=update_target_num,\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results Visualization\n",
    "\n",
    "Plot the training progress to analyze learning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_result(\u001b[43mresults\u001b[49m, interval=\u001b[32m10\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "plot_result(results, interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Evaluation\n",
    "\n",
    "Evaluate the trained agent's performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Custom Agent\n",
      "========================================\n",
      "Games: 1000\n",
      "\n",
      "Running built-in bots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 14:46:10,450\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomBot: Using existing results for all 1000 games\n",
      "LargestGroupBot: Using existing results for all 1000 games\n",
      "GreedySinglesBot: Using existing results for all 1000 games\n",
      "Running agent...\n",
      "Evaluating agent: CNN_simple_reward_base_20251001_144612_9743f32a (ephemeral mode)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 14:46:13,590\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using Ray parallel execution with 1000 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running CNN_simple_reward_base_20251001_144612_9743f32a (parallel): 100%|██████████| 1000/1000 [00:22<00:00, 44.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results computed but not saved (ephemeral evaluation)\n",
      "\n",
      "Performance Results:\n",
      "------------------------------\n",
      "1. GreedySinglesBot\n",
      "   Completion rate: 46.2%\n",
      "   Avg tiles cleared: 62.9\n",
      "   Avg moves made: 12.7\n",
      "   Avg singles remaining: 1.1\n",
      "\n",
      "2. RandomBot\n",
      "   Completion rate: 15.2%\n",
      "   Avg tiles cleared: 61.1\n",
      "   Avg moves made: 13.6\n",
      "   Avg singles remaining: 2.9\n",
      "\n",
      "3. LargestGroupBot\n",
      "   Completion rate: 14.3%\n",
      "   Avg tiles cleared: 61.0\n",
      "   Avg moves made: 11.7\n",
      "   Avg singles remaining: 3.0\n",
      "\n",
      "4. CNN_simple_reward_base\n",
      "   Completion rate: 0.1%\n",
      "   Avg tiles cleared: 17.4\n",
      "   Avg moves made: 99.9\n",
      "   Avg singles remaining: 13.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#benchmarker = Benchmark(config, 1000, storage_type='database')\n",
    "#results = benchmarker.evaluate_agent(agent)\n",
    "#stats = _compute_stats({agent.model_name: results})\n",
    "results = benchmark_agent(agent, config, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Game Visualization (Optional)\n",
    "\n",
    "Watch the trained agent play the game interactively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
